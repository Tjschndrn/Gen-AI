#!/bin/env python
"""
Script for downloading SDSS galaxy image cutouts in parallel using multi-threading
(inspired from https://github.com/thespacedoctor/panstamps)
"""
import os
import logging
import concurrent.futures
import random
import time
import requests
import urllib
from astropy import units as u
import pandas as pd
import csv

def get_SDSS_url(ra, dec, impix=256, imsize=1*u.arcmin):
    cutoutbaseurl = 'http://skyservice.pha.jhu.edu/DR12/ImgCutout/getjpeg.aspx'
    query_string = urllib.parse.urlencode(dict(ra=ra, dec=dec, width=impix, height=impix, scale=imsize.to(u.arcsec).value/impix))
    url = cutoutbaseurl + '?' + query_string
    return url

# a custom generator to create list of lists from pandas dataframe
def chunks(lst, n):
    """yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def fetch_url(entry):
    downloaded = False
    tries = 5
    count = 0
    uri, path = entry
    timeout = gtimeout

    randSleep = random.randint(1, 101) / 20.
    time.sleep(randSleep)

    while not downloaded and count < tries:
        try:
            r = requests.get(uri, stream=True, timeout=timeout)
        except Exception as e:
            print(e)
            count += 1
            timeout *= 2
            llog.warning(f"timeout on attempt number {count}/{tries}. Increasing to {timeout}s")
            continue

        if r.status_code == 200:
            with open(path, 'wb') as f:
                for chunk in r:
                    f.write(chunk)
            return path
        else:
            count += 1
            llog.warning(f"Getting status code {r.status_code} on download attempt {count}/{tries}.")
            downloaded = False

    return None

def multiobject_download(urlList, downloadDirectory, log, filenames, timeStamp=True, timeout=180, concurrentDownloads=10):
    global gtimeout
    global llog
    llog = log
    gtimeout = float(timeout)

    thisArray = []
    totalCount = len(urlList)

    if not isinstance(downloadDirectory, list):
        for i, url in enumerate(urlList):
            filename = filenames[i]
            localFilepath = downloadDirectory + "/" + filename
            thisArray.extend([[url, localFilepath]])

    with concurrent.futures.ThreadPoolExecutor() as executor:
        results = executor.map(fetch_url, thisArray)

    urlNum = 0
    returnPaths = []
    for path in results:
        returnPaths.append(path)
        urlNum += 1
        if urlNum > 1:
            sys.stdout.write("\x1b[1A\x1b[2K")
        percent = (float(urlNum) / float(totalCount)) * 100.
        print("  %(urlNum)s / %(totalCount)s (%(percent)1.1f%%) URLs downloaded" % locals())

    localPaths = []
    localPaths[:] = [o[1] for o in thisArray if o[1] in returnPaths]
    return localPaths


if __name__ == "__main__":
    # Hardcoded paths for CSV and output directory
    csv_path = "barred_rings_gt50.csv"
    output_dir = "output_directory"

    download_list = os.path.join(output_dir, "downloaded.csv")
    url_notfound = os.path.join(output_dir, "urlnotfound.csv")
    downdir = os.path.join(output_dir, "panstamps")

    if not os.path.exists(output_dir):
        print(f"Directory {output_dir} doesn't exist. Creating...")
        os.makedirs(output_dir)
    if not os.path.exists(downdir):
        os.makedirs(downdir)

    print(f"Download list is being appended to {download_list}")

    # Initialize logger
    logger = logging.getLogger("my logger")

    # Read catalogue file into dataframe
    data = pd.read_csv(csv_path)

    # Assuming you know the column names beforehand
    racol = "ra"
    deccol = "dec"
    objidcol = "SDSS_Objid"

    retrieved_ids = []

    try:
        downloaded = pd.read_csv(download_list, names=['paths'], dtype={"paths": str})
        print("[INFO]: Resuming previous download job")
        paths = list(downloaded["paths"])
        if not any(isinstance(item, str) for item in paths):
            print("Found non-strings in the list of paths")
        retrieved_ids = [os.path.basename(x).rstrip(".jpeg") for x in paths if isinstance(x, str)]
        print(f"length of retrieved ids {len(retrieved_ids)}")
    except Exception as e:
        print(f"[INFO]: This is a fresh download... Error: {e}")

    batches = list(chunks(list(map(list, data[[racol, deccol, objidcol]].itertuples(index=False))), 10))

    for batch in batches:
        concurrentDownloads = len(batch)
        objids = []
        allUrls = []
        for row in batch:
            RA = row[0]
            DEC = row[1]
            oid = str(row[2])

            if str(oid) in retrieved_ids:
                print("skipped")
                continue
            objids.append(oid + ".jpeg")

            try:
                url = get_SDSS_url(ra=RA, dec=DEC)
                allUrls.append(url)
            except Exception as e:
                with open(url_notfound, "a") as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow([f"{RA},{DEC} not found"])
                    print(f"Error getting URL for RA={RA}, DEC={DEC}: {e}")

        st = time.time()
        print("len of urls ,", len(allUrls), len(objids))
        localUrls = multiobject_download(
            urlList=allUrls,
            downloadDirectory=downdir,
            log=logger,
            timeout=180,
            concurrentDownloads=concurrentDownloads,
            filenames=objids
        )

        t = time.time() - st
        print(f"Time taken:{t}s")
        with open(download_list, "a") as csvfile:
            writer = csv.writer(csvfile)
            for item in localUrls:
                writer.writerow([item])
